import numpy as np

# Define kernel functions
def quartic_kernel(x):
    return np.where(np.abs(x) <= 1, 15/16 * (1 - x**2)**2, 0)

# quartic Kernel
def quartic_kernel_2d(x, y): 
    z = quartic_kernel(x) * quartic_kernel(y)
    return z


# Define the local polynomial regression function for functional data
def lp_fplsim(data, beta, theta, bandwidth1, bandwidth2, toest, kernel_type='quartic', degree = 1):
    """
    This function performs local polynomial fractional polynomial simulation.

    Parameters:
    -----------
    data : dict 
        The input data for the simulation. It cab be generated by generate_data. For the specific format, please check generate_data.

    beta : float
        The beta coefficient in the FPLSIM.

    theta : float
        The theta parameter in the FPLSIM.

    bandwidth1 : float
        The bandwidth for the first dimension of the data.

    bandwidth2 : float
        The bandwidth for the second dimension of the data.

    toest : tuple
        The points to be estimated. It should be a n*2 matrix for n points of interest.

    kernel_type : str, optional
        The type of kernel to use for the simulation. Default is 'quartic'.

    degree : int, optional
        The degree of the local polynomial. Default is 1.

    Returns:
    --------
    array-like
        The estimation at toest.

    Raises:
    -------
    ValueError
        If the input parameters are not in the expected format or value.

    Examples:
    ---------
    >>> data = generate_data(100,5)
    >>> beta = np.array([1/3, 2/3, 2/3])
    >>> theta = np.array([2,1])
    >>> bandwidth1 = 0.2
    >>> bandwidth2 = 0.2
    >>> t = np.concatenate(data['t'])
    >>> x = np.concatenate(data['x'])
    >>> u = np.dot(x, beta)
    >>> toest = np.column_stack((t,u))
    >>> lp_fplsim(data, beta, theta, bandwidth1, bandwidth2, toest)
    """
    pass

    if kernel_type == 'quartic':
        kernel = quartic_kernel_2d
    else:
        raise ValueError('Unsupported kernel type')
    
    mi = data['mi']
    t = np.concatenate(data['t'])
    x = np.concatenate(data['x'])
    z = np.concatenate(data['z'])
    y = np.concatenate(data['y'])

    xbeta = np.dot(x, beta)
    ztheta = np.dot(z, theta)

    ytilde = y - ztheta
    weights = np.repeat(mi, mi)
    
    # Compute the polynomial coefficients for each x0 value
    beta_hat = []

    for xi1, xi2 in toest:
        # Construct the weight and design matrices
        W = np.diag(kernel(np.abs(t-xi1)/bandwidth1, np.abs(xbeta-xi2)/bandwidth2)
                    /bandwidth1/bandwidth2/weights)
        t_design = np.vander(t - xi1, degree + 1, increasing=True)
        u_design = np.vander(xbeta - xi2, degree + 1, increasing=True)[:, 1:]
        X = np.column_stack((t_design, u_design))
    
        beta_hat_i = np.linalg.inv(X.T @ W @ X) @ X.T @ W @ ytilde
        beta_hat.append(beta_hat_i)
    
    # # Reshape
    beta_hat = np.array(beta_hat)
    
    # return beta_hat_reshaped
    return beta_hat


def loss_fplsim(data, beta, theta, bandwidth1, bandwidth2,  kernel_type='quartic',  degree=1):
    # data should be in the form of (x,z,y), where x is the non-parametric one, z is the linear one, and y is the response
    # each raw is the observation
    t = np.concatenate(data['t'])
    x = np.concatenate(data['x'])
    z = np.concatenate(data['z'])
    y = np.concatenate(data['y'])

    # transformed model: y- \theta^T Z = eta(\beta^T X) + \epsilon
    y_transformed = y - np.dot(z, theta)
    u_transformed = np.dot(x, beta) 

    tx = np.column_stack((t, u_transformed))

    # estimate eta
    eta_hat_d = lp_fplsim(data, beta, theta, bandwidth1, bandwidth2, toest= tx, kernel_type= kernel_type, degree= degree)
    eta_hat = eta_hat_d[:,0]

    loss = np.sum((y_transformed - eta_hat) ** 2)

    return loss

# ### k-fold index sets generator
def k_fold_indices(n, k):
    indices = list(range(n))
    fold_sizes = [n//k + 1 if i < n%k else n//k for i in range(k)]
    current = 0
    result = []
    for fold_size in fold_sizes:
        start, stop = current, current + fold_size
        test_indices = indices[start:stop]
        train_indices = indices[:start] + indices[stop:]
        result.append((train_indices, test_indices))
        current = stop
    return result


def loss_fplsim_kfold(data, beta, theta, bandwidth1, bandwidth2, k=None, kernel_type = 'quartic', degree =1):

    n = len(data['t'])

    if k is None:
        k = n

    kfold = k_fold_indices(n, k)


    losses = []

    for i in range(k):
        train, test = kfold[i]
        ttrain = [data['t'][j] for j in train]
        xtrain = [data['x'][j] for j in train]
        ztrain = [data['z'][j] for j in train]
        ytrain = [data['y'][j] for j in train]
        mitrain = data['mi'][train]
        traindata = {'t':ttrain, 'x':xtrain, 'z':ztrain, 'y':ytrain, 'mi':mitrain}

        ttest = [data['t'][j] for j in test]
        xtest = [data['x'][j] for j in test]
        ztest = [data['z'][j] for j in test]
        ytest = [data['y'][j] for j in test]

        ttest = np.concatenate(ttest)
        xbetatest = np.dot(np.concatenate(xtest), beta)
        zthetatest = np.dot(np.concatenate(ztest), theta)
        ytest = np.concatenate(ytest)

        toest = np.column_stack((ttest, xbetatest))

        est = lp_fplsim(traindata, beta, theta, bandwidth1, bandwidth2, toest, kernel_type= kernel_type, degree=degree)[:,0]

        lossi = np.sum((ytest - zthetatest - est)**2)
        
        losses.append(lossi)

    totalloss = np.sum(np.array(losses))

    return totalloss
    



# Profile least sequare estimation
from scipy.optimize import minimize

def paraest_fplsim_h(data, bandwidth1,bandwidth2, kernel_type= 'quartic', degree=1):
    x = np.concatenate(data['x'])
    z = np.concatenate(data['z'])

    px = x.shape[1]
    pz = z.shape[1]

    # define the objective function
    def objective(params):
        try:
            beta = params[:px]
            theta = params[px:]
            loss = loss_fplsim(data, beta, theta, bandwidth1, bandwidth2, kernel_type=kernel_type, degree=degree)
        except:
            # return a large value if an error occurs
            loss = np.inf
        return loss
    
    # define the constraints
    cons = ({'type': 'eq', 'fun': lambda params: np.linalg.norm(params[:px]) - 1},
        {'type': 'ineq', 'fun': lambda params: params[0]})

    # define the constraint
    # cons = ({'type': 'eq', 'fun': lambda params: np.linalg.norm(params[:px]) - 1})
    
    # define the initial values for beta and theta
    beta_init = np.ones(px)/ np.sqrt(px)
    # beta_init = np.array([1/3, -2/3, 2/3])
    theta_init = np.ones(pz)

    params_init = np.concatenate((beta_init, theta_init))

    # minimize the objective function
    res = minimize(objective, params_init, method='SLSQP', constraints=cons, options={'ftol':1e-6})

    # extract the optimal values for beta and theta
    beta_opt = res.x[:px]
    theta_opt = res.x[px:]
    
    # return beta_opt, theta_opt, bandwidth1, bandwidth2
    return beta_opt, theta_opt

def paraest_fplsim(data, kernel_type='quartic', degree=1, k= 10):
    x = np.concatenate(data['x'])
    z = np.concatenate(data['z'])

    n = len(data['t'])

    px = x.shape[1]
    pz = z.shape[1]

    # define the objective function
    def objective(params):
        try:
            bandwidth1 = params[0]
            bandwidth2 = params[1]
            beta = params[2:(px+2)]
            theta = params[(px+2):]
            loss = loss_fplsim_kfold(data, beta, theta, bandwidth1, bandwidth2, k, kernel_type, degree)
        except:
            # return a large value if an error occurs
            loss = np.inf
        return loss
    

    # define the constraints
    cons = ({'type': 'eq', 'fun': lambda params: np.linalg.norm(params[2:(px+2)]) - 1},
        {'type': 'ineq', 'fun': lambda params: params[2]},
        {'type': 'ineq', 'fun': lambda params: params[0]-(5/n)},
        {'type': 'ineq', 'fun': lambda params: params[1]-(5/n)})

    # define the constraint
    # cons = ({'type': 'eq', 'fun': lambda params: np.linalg.norm(params[:px]) - 1})
    
    # define the initial values for beta and theta
    b1_init = 0.3
    b2_init = 0.3
    beta_init = np.ones(px)/ np.sqrt(px)
    theta_init = np.ones(pz)

    params_init = np.concatenate(([b1_init, b2_init], beta_init, theta_init))

    # minimize the objective function
    res = minimize(objective, params_init, method='SLSQP', constraints=cons)

    # extract the optimal values for beta and theta
    bandwidth1_opt = res.x[0]
    bandwidth2_opt = res.x[1]
    beta_opt = res.x[2:(px+2)]
    theta_opt = res.x[(px+2):]
    
    # return beta_opt, theta_opt, bandwidth1, bandwidth2
    return bandwidth1_opt, bandwidth2_opt, beta_opt, theta_opt

# from datagenerate import *

# beta_0 = np.array([1/3, -2/3, 2/3])
# beta_1 = np.array([1/3, -2/3, 2/3]) + 0.01
# beta_2 = np.array([0.6651245, -0.73436453,  0.13534543])
# theta_0 = np.array([2,1])
# data = generate_data(200,5)
# b1 = 0.1
# b2 = 0.2
# # beta_opt, theta_opt = optimize_plsim_h_2d(data, bandwidth1= 0.5, bandwidth2=0.5)
# loss0 = loss_plsim_2d(data, b1,  b2, beta_0, theta_0)
# loss1 = loss_plsim_2d(data, b1,  b2, beta_1, theta_0)
# loss2= loss_plsim_2d(data, b1,  b2, beta_2, theta_0)
# print(loss0)
# print(loss1)
# print(loss2)
# print(beta_opt, theta_opt)
